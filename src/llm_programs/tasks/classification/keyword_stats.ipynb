{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffee1b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "import re\n",
    "\n",
    "def compile_keyword_patterns(keywords):\n",
    "    return [re.compile(rf\"\\b{re.escape(kw)}\\b\", re.IGNORECASE) for kw in keywords]\n",
    "\n",
    "# Define full keyword list\n",
    "KEYWORDS = [\n",
    "    \"automated processing\",\n",
    "    \"profiling\",\n",
    "    \"artificial intelligence\",\n",
    "    \"ai\",\n",
    "    \"machine learning\",\n",
    "    \"deep learning\",\n",
    "    \"neural network\",\n",
    "    \"algorithm\",\n",
    "    \"intelligent system\",\n",
    "    \"intelligent systems\",\n",
    "    \"automated system\",\n",
    "    \"automated systems\",\n",
    "    \"decision-making\",\n",
    "    \"automated decision\",\n",
    "    \"automated decision-making\",\n",
    "    \"autonomous system\",\n",
    "    \"autonomous systems\",\n",
    "    \"predictive model\",\n",
    "    \"predictive analytics\",\n",
    "    \"computer vision\",\n",
    "    \"natural language processing\",\n",
    "    \"nlp\",\n",
    "    \"data-driven\",\n",
    "    \"learning system\",\n",
    "    \"self-learning\",\n",
    "    \"self-learning system\",\n",
    "    \"pattern recognition\",\n",
    "    \"classification model\",\n",
    "    \"generative model\",\n",
    "    \"chatbot\",\n",
    "    \"large language model\",\n",
    "    \"language model\",\n",
    "    \"reinforcement learning\",\n",
    "    \"unsupervised learning\",\n",
    "    \"supervised learning\",\n",
    "    \"training data\",\n",
    "    \"training dataset\",\n",
    "    \"ai system\",\n",
    "    \"ai-based\",\n",
    "    \"ml-based\",\n",
    "    \"intelligent automation\",\n",
    "    \"cognitive computing\",\n",
    "    \"robotic process automation\",\n",
    "    \"rpa\",\n",
    "    \"digital agent\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93567117",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def count_keywords_in_paragraph(paragraph, compiled_patterns):\n",
    "    keyword_counts = defaultdict(int)\n",
    "    matched_keywords = set()\n",
    "\n",
    "    for pattern in compiled_patterns:\n",
    "        matches = pattern.findall(paragraph)\n",
    "        if matches:\n",
    "            keyword = pattern.pattern.strip(r\"\\b\").lower()  # extract clean version\n",
    "            keyword_counts[keyword] += len(matches)\n",
    "            matched_keywords.add(keyword)\n",
    "\n",
    "    return keyword_counts, matched_keywords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f49a5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_keyword_stats(base_dir):\n",
    "    stats = {\n",
    "        \"total_paragraphs\": 0,\n",
    "        \"keyword_occurrences\": defaultdict(int),\n",
    "        \"keyword_paragraph_hits\": defaultdict(int)\n",
    "    }\n",
    "\n",
    "    base_path = Path(base_dir)\n",
    "\n",
    "    for segment_dir in base_path.iterdir():\n",
    "        if not segment_dir.is_dir():\n",
    "            continue\n",
    "\n",
    "        for txt_file in segment_dir.glob(\"*.txt\"):\n",
    "            with open(txt_file, 'r', encoding='utf-8') as f:\n",
    "                paragraph = f.read()\n",
    "                stats[\"total_paragraphs\"] += 1\n",
    "\n",
    "                compiled_patterns = compile_keyword_patterns(KEYWORDS)\n",
    "\n",
    "                counts, matched_keywords = count_keywords_in_paragraph(paragraph, compiled_patterns)\n",
    "\n",
    "                # Add total word count\n",
    "                for kw, count in counts.items():\n",
    "                    stats[\"keyword_occurrences\"][kw] += count\n",
    "\n",
    "                # Track per-paragraph presence\n",
    "                for kw in matched_keywords:\n",
    "                    stats[\"keyword_paragraph_hits\"][kw] += 1\n",
    "\n",
    "    stats[\"keyword_occurrences\"] = dict(stats[\"keyword_occurrences\"])\n",
    "    stats[\"keyword_paragraph_hits\"] = dict(stats[\"keyword_paragraph_hits\"])\n",
    "\n",
    "    return stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7774fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # base_dir = \"fichiers segmentÃ©s\"\n",
    "    base_dir = \"../contracts_3_redacted\"\n",
    "    output_file = \"keyword_stats.json\"\n",
    "\n",
    "    stats = run_keyword_stats(base_dir)\n",
    "\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(stats, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    print(f\"âœ… Keyword statistics saved to {output_file}\")\n",
    "    print(f\"ðŸ“Š Scanned {stats['total_paragraphs']} paragraphs.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f46f1b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
